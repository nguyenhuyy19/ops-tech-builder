PROMPTS COMPARISON

When using the same Run Prompts on the different models likes GPT and Claude (or describe expected differences). Often there will be characteristics and properties of the model on the strengths of weaknesses in giving the following results:
    On GPT (gpt-4, gpt-3.5-turbo)
        - Following clear instructions for how you want the answer structured.
        - Understanding and working with consistent formatting, especially if you ask for JSON.
        - More organized answers, gives shorter, often with focus on the main point.
        - Give direct answers without showing much of its thought process.
        Can something: 
        - Make up information or grab wrong words and context unless you're tell it not to.
    On Claude 
        - Understanding the overall meaning and figuring out what kind of information you're looking for (like knowing if something is a person or a company).
        - More accurate in telling the difference between people and organizations.
        - Conversational and uses paragraphs with detailed explanations.
        - Has its own different safety rules and might handle tricky requests differently
        Can somthing: 
        - Very long answers unless you give it examples and unless you're very specific


NORMALIZE THE OUTPUT 

In this Normalizing Model Output to make sure the information from both models is consistent and easy to use efficiency
- I have take the raw text output from the models and process, this helps turn the text into a structured data format and easy a backup plan to fix or clean it up so you can still use the information.
- Format Standardization will also help focus on the main ideas to be searched.
- Content Extraction thoroughly, extract key facts for comparison and Focus on substantive content rather than stylistic differences.
- Establishing consistent evaluation criteria that focus on the underlying quality and usefulness of responses rather than surface-level stylistic differences.
